"""
ÌååÏùº ÏóÖÎ°úÎìú API ÎùºÏö∞ÌÑ∞
MODIFIED 2024-01-20: ÌååÏùº raw text Ï°∞Ìöå Î∞è Ìé∏Ïßë Í∏∞Îä• Ï∂îÍ∞Ä
ENHANCED 2024-01-21: ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ Í∏∞Îä• Ï∂îÍ∞Ä
REFACTORED 2024-01-21: Ï§ëÎ≥µ Í≤ÄÏÉâ API Ï†úÍ±∞ Î∞è ÏΩîÎìú Ï†ïÎ¶¨
FIXED 2024-01-21: import Í≤ΩÎ°ú ÏàòÏ†ï (file_processing -> data_processing)
FIXED 2025-06-03: DocumentProcessor Ï¥àÍ∏∞Ìôî Î∞è Î©îÏÑúÎìú Ìò∏Ï∂ú Ïò§Î•ò ÏàòÏ†ï
"""
import os
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from fastapi import APIRouter, File, Form, HTTPException, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from database.connection import get_database
from data_processing.document_processor import DocumentProcessor
from retrieval.vector_search import VectorSearch
from utils.logger import get_logger

logger = get_logger(__name__)
router = APIRouter()

class UploadResponse(BaseModel):
    """ÏóÖÎ°úÎìú ÏùëÎãµ Î™®Îç∏"""
    success: bool
    message: str
    file_id: str
    original_filename: str
    processed_chunks: int
    storage_path: Optional[str] = None

class FileStatus(BaseModel):
    """ÌååÏùº ÏÉÅÌÉú Î™®Îç∏"""
    file_id: str
    original_filename: str
    file_type: str
    file_size: int
    status: str  # 'uploading', 'processing', 'completed', 'failed'
    processed_chunks: int
    upload_time: datetime
    folder_id: Optional[str] = None

class FileSearchRequest(BaseModel):
    """ÌååÏùº Í≤ÄÏÉâ ÏöîÏ≤≠ Î™®Îç∏"""
    query: str
    search_type: str = "both"  # filename, content, both
    folder_id: Optional[str] = None
    limit: int = 20
    skip: int = 0

class FileSearchResult(BaseModel):
    """ÌååÏùº Í≤ÄÏÉâ Í≤∞Í≥º Î™®Îç∏"""
    file_id: str
    original_filename: str
    file_type: str
    file_size: int
    processed_chunks: int
    upload_time: datetime
    folder_id: Optional[str] = None
    description: Optional[str] = None
    match_type: str  # filename, content, both
    relevance_score: float
    matched_content: Optional[str] = None  # Í≤ÄÏÉâÏñ¥ÏôÄ Îß§Ïπ≠Îêú ÎÇ¥Ïö© ÎØ∏Î¶¨Î≥¥Í∏∞

class FileSearchResponse(BaseModel):
    """ÌååÏùº Í≤ÄÏÉâ ÏùëÎãµ Î™®Îç∏"""
    files: List[FileSearchResult]
    total_found: int
    query: str
    search_type: str
    execution_time: float

class FileUpdateRequest(BaseModel):
    """ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ ÏöîÏ≤≠ Î™®Îç∏"""
    filename: Optional[str] = None
    description: Optional[str] = None
    folder_id: Optional[str] = None

class FilePreviewResponse(BaseModel):
    """ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ ÏùëÎãµ Î™®Îç∏"""
    file_id: str
    original_filename: str
    file_type: str
    preview_text: str
    preview_length: int
    total_length: int
    has_more: bool
    preview_type: str  # "text", "pdf_extract", "document_extract"

@router.post("/", response_model=UploadResponse)
async def upload_file(
    file: UploadFile = File(...),
    folder_id: Optional[str] = Form(None),
    description: Optional[str] = Form(None)
):
    """ÌååÏùº ÏóÖÎ°úÎìú Î∞è Ï≤òÎ¶¨"""
    try:
        # ÎîîÎ≤ÑÍπÖ: Î∞õÏùÄ Ìèº Îç∞Ïù¥ÌÑ∞ Î°úÍ∑∏ Ï∂úÎ†•
        logger.info(f"ÏóÖÎ°úÎìú Ìèº Îç∞Ïù¥ÌÑ∞ - ÌååÏùºÎ™Ö: {file.filename}, folder_id: '{folder_id}', description: '{description}'")
        
        # ÌååÏùº Ï†ïÎ≥¥ Í≤ÄÏ¶ù
        if not file.filename:
            raise HTTPException(status_code=400, detail="ÌååÏùºÎ™ÖÏù¥ ÏóÜÏäµÎãàÎã§.")
        
        # ÌååÏùº ÌÉÄÏûÖ ÌôïÏù∏
        allowed_types = ['.txt', '.pdf', '.docx', '.doc', '.md']
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_types:
            raise HTTPException(
                status_code=400, 
                detail=f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. ÏßÄÏõê ÌòïÏãù: {', '.join(allowed_types)}"
            )
        
        # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏ (10MB Ï†úÌïú)
        file_content = await file.read()
        if len(file_content) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(status_code=400, detail="ÌååÏùº ÌÅ¨Í∏∞Îäî 10MBÎ•º Ï¥àÍ≥ºÌï† Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # ÌååÏùºÏùÑ Îã§Ïãú Ï≤òÏùåÏúºÎ°ú ÎêòÎèåÎ¶¨Í∏∞
        await file.seek(0)
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞
        db = await get_database()
        
        # ÌååÏùº ID ÏÉùÏÑ±
        file_id = str(uuid.uuid4())
        
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        upload_dir = Path("uploads")
        upload_dir.mkdir(exist_ok=True)
        
        temp_filename = f"{file_id}_{file.filename}"
        temp_file_path = upload_dir / temp_filename
        
        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(file_content)
        
        # Form Îç∞Ïù¥ÌÑ∞ Ï†ïÎ¶¨ - Îπà Í∞íÏù¥ÎÇò Í∏∞Î≥∏Í∞í Ï≤òÎ¶¨
        clean_folder_id = None
        clean_description = None
        
        if folder_id and folder_id.strip() and folder_id not in ["string", "null"]:
            clean_folder_id = folder_id.strip()
        
        if description and description.strip() and description not in ["string", "null"]:
            clean_description = description.strip()
        
        logger.info(f"Ï†ïÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ - clean_folder_id: '{clean_folder_id}', clean_description: '{clean_description}'")
        
        # ÌååÏùº Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        file_metadata = {
            "file_id": file_id,
            "original_filename": file.filename,
            "file_type": file_ext[1:],  # Ï†ê Ï†úÍ±∞
            "file_size": len(file_content),
            "upload_time": datetime.utcnow(),
            "folder_id": clean_folder_id,
            "description": clean_description
        }
        
        # Î¨∏ÏÑú Ï≤òÎ¶¨Í∏∞Î°ú ÌååÏùº Ï≤òÎ¶¨
        processor = DocumentProcessor(db)
        result = await processor.process_and_store(
            file_path=temp_file_path,
            file_metadata=file_metadata
        )
        
        # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
        try:
            temp_file_path.unlink()
        except Exception as e:
            logger.warning(f"ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {e}")
        
        logger.info(f"ÌååÏùº ÏóÖÎ°úÎìú ÏôÑÎ£å: {file.filename} -> {file_id}")
        
        return UploadResponse(
            success=True,
            message="ÌååÏùº ÏóÖÎ°úÎìúÍ∞Ä ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.",
            file_id=file_id,
            original_filename=file.filename,
            processed_chunks=result["chunks_count"],
            storage_path=str(temp_file_path)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÏóÖÎ°úÎìú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌååÏùº ÏóÖÎ°úÎìú Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.get("/status/{file_id}", response_model=FileStatus)
async def get_file_status(file_id: str):
    """ÌååÏùº Ï≤òÎ¶¨ ÏÉÅÌÉú Ï°∞Ìöå"""
    try:
        db = await get_database()
        
        # documents Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå
        document = await db.documents.find_one({"file_id": file_id})
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # chunks Ïª¨Î†âÏÖòÏóêÏÑú Ï≤≠ÌÅ¨ Í∞úÏàò Ï°∞Ìöå
        chunks_count = await db.chunks.count_documents({"file_id": file_id})
        
        return FileStatus(
            file_id=file_id,
            original_filename=document["original_filename"],
            file_type=document["file_type"],
            file_size=document["file_size"],
            status="completed",  # ÌòÑÏû¨Îäî Îã®ÏàúÌôî
            processed_chunks=chunks_count,
            upload_time=document["upload_time"],
            folder_id=document.get("folder_id")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/search", response_model=FileSearchResponse)
async def search_files(request: FileSearchRequest):
    """üìç ÏûêÏó∞Ïñ¥ ÌååÏùº Í≤ÄÏÉâ - ÌååÏùºÎ™ÖÍ≥º ÎÇ¥Ïö©ÏúºÎ°ú Í≤ÄÏÉâ Í∞ÄÎä•"""
    try:
        start_time = time.time()
        db = await get_database()
        
        # 1. Í∏∞Î≥∏ ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÑ§Ï†ï (folder_idÍ∞Ä Ïã§Ï†ú Í∞íÏùº ÎïåÎßå Ï†ÅÏö©)
        base_filter = {}
        if request.folder_id and request.folder_id.strip() and request.folder_id != "string":
            base_filter["folder_id"] = request.folder_id
        
        found_files = []
        
        # 2. ÌååÏùºÎ™Ö Í≤ÄÏÉâ (filename ÎòêÎäî both)
        if request.search_type in ["filename", "both"]:
            filename_filter = base_filter.copy()
            filename_filter["original_filename"] = {"$regex": request.query, "$options": "i"}
            
            filename_docs = await db.documents.find(filename_filter).to_list(None)
            
            for doc in filename_docs:
                chunks_count = await db.chunks.count_documents({"file_id": doc["file_id"]})
                
                found_files.append({
                    "file_id": doc["file_id"],
                    "original_filename": doc["original_filename"],
                    "file_type": doc["file_type"],
                    "file_size": doc["file_size"],
                    "processed_chunks": chunks_count,
                    "upload_time": doc["upload_time"],
                    "folder_id": doc.get("folder_id"),
                    "description": doc.get("description"),
                    "match_type": "filename",
                    "relevance_score": 1.0,  # ÌååÏùºÎ™Ö Îß§ÏπòÎäî ÎÜíÏùÄ Ï†êÏàò
                    "matched_content": f"ÌååÏùºÎ™Ö Îß§Ïπò: {doc['original_filename']}"
                })
        
        # 3. ÎÇ¥Ïö© Í≤ÄÏÉâ (content ÎòêÎäî both)
        if request.search_type in ["content", "both"]:
            # folder_id ÌïÑÌÑ∞ÎßÅÏùÑ ÏúÑÌï¥ documentsÏôÄ Ï°∞Ïù∏
            pipeline = [
                {
                    "$lookup": {
                        "from": "documents",
                        "localField": "file_id",
                        "foreignField": "file_id",
                        "as": "document_info"
                    }
                },
                {"$unwind": "$document_info"}
            ]
            
            # folder_id ÌïÑÌÑ∞ Ï∂îÍ∞Ä (Ïã§Ï†ú Í∞íÏùº ÎïåÎßå)
            if request.folder_id and request.folder_id.strip() and request.folder_id != "string":
                pipeline.append({
                    "$match": {"document_info.folder_id": request.folder_id}
                })
            
            # ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ Ï∂îÍ∞Ä
            pipeline.extend([
                {"$match": {"text": {"$regex": request.query, "$options": "i"}}},
                {"$limit": request.limit * 2}  # Îçî ÎßéÏù¥ Ï∞æÏïÑÏÑú ÎÇòÏ§ëÏóê ÌååÏùºÎ≥ÑÎ°ú Í∑∏Î£πÌôî
            ])
            
            content_chunks = await db.chunks.aggregate(pipeline).to_list(None)
            
            # ÌååÏùºÎ≥ÑÎ°ú Í∑∏Î£πÌôîÌïòÍ≥† ÏµúÍ≥† Îß§Ïπ≠ Ï≤≠ÌÅ¨Îßå ÎÇ®Í∏∞Í∏∞
            file_matches = {}
            for chunk in content_chunks:
                file_id = chunk["file_id"]
                doc_info = chunk["document_info"]
                
                if file_id not in file_matches:
                    chunks_count = await db.chunks.count_documents({"file_id": file_id})
                    
                    # Îß§Ïπ≠Îêú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (Í∞ÑÎã®Ìïú ÌïòÏù¥ÎùºÏù¥Ìä∏)
                    text = chunk["text"]
                    query_lower = request.query.lower()
                    text_lower = text.lower()
                    
                    # Í≤ÄÏÉâÏñ¥ Ï£ºÎ≥Ä ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                    match_index = text_lower.find(query_lower)
                    if match_index != -1:
                        start = max(0, match_index - 50)
                        end = min(len(text), match_index + len(request.query) + 50)
                        matched_content = "..." + text[start:end] + "..."
                    else:
                        matched_content = text[:100] + "..."
                    
                    file_matches[file_id] = {
                        "file_id": file_id,
                        "original_filename": doc_info["original_filename"],
                        "file_type": doc_info["file_type"],
                        "file_size": doc_info["file_size"],
                        "processed_chunks": chunks_count,
                        "upload_time": doc_info["upload_time"],
                        "folder_id": doc_info.get("folder_id"),
                        "description": doc_info.get("description"),
                        "match_type": "content",
                        "relevance_score": 0.8,  # ÎÇ¥Ïö© Îß§ÏπòÎäî Ï§ëÍ∞Ñ Ï†êÏàò
                        "matched_content": matched_content
                    }
            
            found_files.extend(file_matches.values())
        
        # 4. Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è Ï†ïÎ†¨ (file_id Í∏∞Ï§Ä)
        unique_files = {}
        for file_data in found_files:
            file_id = file_data["file_id"]
            if file_id not in unique_files or file_data["relevance_score"] > unique_files[file_id]["relevance_score"]:
                unique_files[file_id] = file_data
        
        # Í¥ÄÎ†®ÏÑ± Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨
        sorted_files = sorted(unique_files.values(), key=lambda x: x["relevance_score"], reverse=True)
        total_found = len(sorted_files)
        
        # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò Ï†ÅÏö©
        paginated_files = sorted_files[request.skip:request.skip + request.limit]
        
        # 5. ÏùëÎãµ Î™®Îç∏Ïóê ÎßûÍ≤å Î≥ÄÌôò
        search_results = []
        for file_data in paginated_files:
            search_results.append(FileSearchResult(**file_data))
        
        execution_time = time.time() - start_time
        
        # ÎîîÎ≤ÑÍ∑∏ Ï†ïÎ≥¥ Î°úÍπÖ
        logger.info(f"Í≤ÄÏÉâ ÏôÑÎ£å - ÏøºÎ¶¨: '{request.query}', ÌÉÄÏûÖ: {request.search_type}, Ìè¥Îçî: {request.folder_id}, Í≤∞Í≥º: {total_found}Í∞ú")
        
        return FileSearchResponse(
            files=search_results,
            total_found=total_found,
            query=request.query,
            search_type=request.search_type,
            execution_time=round(execution_time, 3)
        )
        
    except Exception as e:
        logger.error(f"ÌååÏùº Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌååÏùº Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.delete("/{file_id}")
async def delete_file(file_id: str):
    """ÏóÖÎ°úÎìúÎêú ÌååÏùº Î∞è Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú"""
    try:
        db = await get_database()
        
        # Î¨∏ÏÑú Ï†ïÎ≥¥ ÌôïÏù∏
        document = await db.documents.find_one({"file_id": file_id})
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # Í¥ÄÎ†® Ï≤≠ÌÅ¨Îì§ ÏÇ≠Ï†ú
        chunks_result = await db.chunks.delete_many({"file_id": file_id})
        
        # Î¨∏ÏÑú Ï†ïÎ≥¥ ÏÇ≠Ï†ú
        doc_result = await db.documents.delete_one({"file_id": file_id})
        
        logger.info(f"ÌååÏùº ÏÇ≠Ï†ú ÏôÑÎ£å: {file_id}, Ï≤≠ÌÅ¨ {chunks_result.deleted_count}Í∞ú, Î¨∏ÏÑú {doc_result.deleted_count}Í∞ú")
        
        return {
            "success": True,
            "message": "ÌååÏùºÏù¥ ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.",
            "deleted_chunks": chunks_result.deleted_count
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/list")
async def list_files(folder_id: Optional[str] = None, limit: int = 50, skip: int = 0):
    """ÏóÖÎ°úÎìúÎêú ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
    try:
        db = await get_database()
        
        # ÌïÑÌÑ∞ Ï°∞Í±¥
        filter_dict = {}
        if folder_id:
            filter_dict["folder_id"] = folder_id
        
        # Î¨∏ÏÑú Î™©Î°ù Ï°∞Ìöå
        cursor = db.documents.find(filter_dict).sort("upload_time", -1).skip(skip).limit(limit)
        documents = await cursor.to_list(None)
        
        # Í∞Å Î¨∏ÏÑúÏùò Ï≤≠ÌÅ¨ Í∞úÏàò Ï°∞Ìöå
        result = []
        for doc in documents:
            chunks_count = await db.chunks.count_documents({"file_id": doc["file_id"]})
            
            result.append({
                "file_id": doc["file_id"],
                "original_filename": doc["original_filename"],
                "file_type": doc["file_type"],
                "file_size": doc["file_size"],
                "processed_chunks": chunks_count,
                "upload_time": doc["upload_time"],
                "folder_id": doc.get("folder_id"),
                "description": doc.get("description")
            })
        
        return {
            "files": result,
            "total": len(result),
            "skip": skip,
            "limit": limit
        }
        
    except Exception as e:
        logger.error(f"ÌååÏùº Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/semantic-search")
async def semantic_search_files(
    q: str,  # Í≤ÄÏÉâÏñ¥
    k: int = 5,  # Í≤∞Í≥º Í∞úÏàò
    folder_id: Optional[str] = None
):
    """üß† AI Í∏∞Î∞ò ÏùòÎØ∏ Í≤ÄÏÉâ - Î≤°ÌÑ∞ Ïú†ÏÇ¨ÎèÑÎ°ú ÌååÏùº Ï∞æÍ∏∞"""
    try:
        db = await get_database()
        vector_search = VectorSearch(db)
        
        # ÌïÑÌÑ∞ Ï°∞Í±¥ ÏÑ§Ï†ï
        filter_dict = {}
        if folder_id and folder_id.strip():  # NoneÏù¥Í±∞ÎÇò Îπà Î¨∏ÏûêÏó¥Ïù¥ ÏïÑÎãê ÎïåÎßå
            filter_dict["folder_id"] = folder_id
        
        # Î≤°ÌÑ∞ Í≤ÄÏÉâ Ïã§Ìñâ
        search_results = await vector_search.search_similar(
            query=q,
            k=k * 3,  # Îçî ÎßéÏù¥ Ï∞æÏïÑÏÑú ÌååÏùºÎ≥ÑÎ°ú Í∑∏Î£πÌôî
            filter_dict=filter_dict
        )
        
        # ÌååÏùºÎ≥ÑÎ°ú Í∑∏Î£πÌôîÌïòÍ≥† ÏµúÍ≥† Ï†êÏàòÎßå ÎÇ®Í∏∞Í∏∞
        file_groups = {}
        for result in search_results:
            chunk = result.get("chunk", {})
            document = result.get("document", {})
            score = result.get("score", 0.0)
            file_id = chunk.get("file_id")
            
            if file_id and (file_id not in file_groups or score > file_groups[file_id]["relevance_score"]):
                # chunks Í∞úÏàò Ï°∞Ìöå
                chunks_count = await db.chunks.count_documents({"file_id": file_id})
                
                file_groups[file_id] = {
                    "file_id": file_id,
                    "original_filename": document.get("original_filename", "Ïïå Ïàò ÏóÜÎäî ÌååÏùº"),
                    "file_type": document.get("file_type", "unknown"),
                    "file_size": document.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": document.get("upload_time"),
                    "folder_id": document.get("folder_id"),
                    "description": document.get("description"),
                    "match_type": "semantic",
                    "relevance_score": score,
                    "matched_content": chunk.get("text", "")[:200] + "..."
                }
        
        # Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨ÌïòÍ≥† ÏÉÅÏúÑ kÍ∞úÎßå Î∞òÌôò
        sorted_files = sorted(file_groups.values(), key=lambda x: x["relevance_score"], reverse=True)
        top_files = sorted_files[:k]
        
        # ÏùëÎãµ ÏÉùÏÑ±
        search_results = [FileSearchResult(**file_data) for file_data in top_files]
        
        return FileSearchResponse(
            files=search_results,
            total_found=len(search_results),
            query=q,
            search_type="semantic",
            execution_time=0.0  # Ïã§Ï†ú ÏãúÍ∞Ñ Ï∏°Ï†ïÏùÄ ÏÉùÎûµ
        )
        
    except Exception as e:
        logger.error(f"ÏùòÎØ∏ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏùòÎØ∏ Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

@router.get("/content/{file_id}")
async def get_file_content(file_id: str):
    """ÌååÏùºÏùò ÏõêÎ≥∏ ÌÖçÏä§Ìä∏ ÎÇ¥Ïö© Ï°∞Ìöå (ÌÜ†Í∏Ä ÌëúÏãúÏö©)"""
    try:
        db = await get_database()
        
        # documents Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Î∞è ÌÖçÏä§Ìä∏ Ï°∞Ìöå
        document = await db.documents.find_one(
            {"file_id": file_id},
            {"original_filename": 1, "raw_text": 1, "processed_text": 1, "file_type": 1, "upload_time": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        return {
            "file_id": file_id,
            "original_filename": document["original_filename"],
            "file_type": document["file_type"],
            "upload_time": document["upload_time"],
            "raw_text": document.get("raw_text", ""),
            "processed_text": document.get("processed_text", ""),
            "text_length": len(document.get("raw_text", ""))
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÎÇ¥Ïö© Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{file_id}")
async def update_file_info(file_id: str, request: FileUpdateRequest):
    """ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ (ÌååÏùºÎ™Ö, ÏÑ§Î™Ö, Ìè¥Îçî Îì±)"""
    try:
        db = await get_database()
        
        # ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÌïÑÎìú Ï§ÄÎπÑ
        update_fields = {}
        if request.filename is not None and request.filename.strip():
            update_fields["original_filename"] = request.filename.strip()
        if request.description is not None:
            update_fields["description"] = request.description.strip() if request.description.strip() else None
        if request.folder_id is not None:
            update_fields["folder_id"] = request.folder_id.strip() if request.folder_id.strip() else None
        
        if not update_fields:
            raise HTTPException(status_code=400, detail="ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§.")
        
        # ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏
        document = await db.documents.find_one({"file_id": file_id})
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # Î¨∏ÏÑú ÏóÖÎç∞Ïù¥Ìä∏
        result = await db.documents.update_one(
            {"file_id": file_id},
            {"$set": update_fields}
        )
        
        if result.modified_count == 0:
            logger.warning(f"ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ Í≤∞Í≥º ÏóÜÏùå: {file_id}")
        
        # Ìè¥Îçî Î≥ÄÍ≤ΩÏãú chunksÏùò metadataÎèÑ ÏóÖÎç∞Ïù¥Ìä∏
        if "folder_id" in update_fields:
            await db.chunks.update_many(
                {"file_id": file_id},
                {"$set": {"metadata.folder_id": update_fields["folder_id"]}}
            )
        
        logger.info(f"ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {file_id} - {update_fields}")
        
        return {
            "success": True,
            "message": "ÌååÏùº Ï†ïÎ≥¥Í∞Ä ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏóàÏäµÎãàÎã§.",
            "updated_fields": update_fields
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/preview/{file_id}")
async def get_file_preview(file_id: str, max_length: int = 500):
    """ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ - Ï≤òÏùå Î™á Ï§ÑÏùò ÌÖçÏä§Ìä∏Î•º Î∞òÌôò"""
    try:
        db = await get_database()
        
        # documents Ïª¨Î†âÏÖòÏóêÏÑú ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå
        document = await db.documents.find_one(
            {"file_id": file_id},
            {"original_filename": 1, "raw_text": 1, "file_type": 1, "text_length": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        raw_text = document.get("raw_text", "")
        file_type = document.get("file_type", "unknown")
        total_length = len(raw_text)
        
        # ÎØ∏Î¶¨Î≥¥Í∏∞ ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        preview_text = ""
        preview_type = "text"
        
        if raw_text:
            # ÌÖçÏä§Ìä∏Î•º Ï§Ñ Îã®ÏúÑÎ°ú Î∂ÑÌï†ÌïòÏó¨ ÏûêÏó∞Ïä§Îü¨Ïö¥ ÎØ∏Î¶¨Î≥¥Í∏∞ ÏÉùÏÑ±
            lines = raw_text.split('\n')
            current_length = 0
            preview_lines = []
            
            for line in lines:
                if current_length + len(line) > max_length:
                    break
                preview_lines.append(line)
                current_length += len(line) + 1  # +1 for newline
            
            preview_text = '\n'.join(preview_lines)
            
            # ÌååÏùº ÌÉÄÏûÖÏóê Îî∞Î•∏ ÎØ∏Î¶¨Î≥¥Í∏∞ ÌÉÄÏûÖ Í≤∞Ï†ï
            if file_type == "pdf":
                preview_type = "pdf_extract"
            elif file_type in ["docx", "doc"]:
                preview_type = "document_extract"
            else:
                preview_type = "text"
        else:
            preview_text = "ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§."
        
        preview_length = len(preview_text)
        has_more = total_length > preview_length
        
        return FilePreviewResponse(
            file_id=file_id,
            original_filename=document["original_filename"],
            file_type=file_type,
            preview_text=preview_text,
            preview_length=preview_length,
            total_length=total_length,
            has_more=has_more,
            preview_type=preview_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/preview/chunks/{file_id}")
async def get_file_preview_with_chunks(file_id: str, chunk_count: int = 3):
    """Ï≤≠ÌÅ¨ Í∏∞Î∞ò ÌååÏùº ÎØ∏Î¶¨Î≥¥Í∏∞ - Ï≤òÏùå Î™á Í∞ú Ï≤≠ÌÅ¨Ïùò ÎÇ¥Ïö©"""
    try:
        db = await get_database()
        
        # ÌååÏùº Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï°∞Ìöå
        document = await db.documents.find_one(
            {"file_id": file_id},
            {"original_filename": 1, "file_type": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # Ï≤òÏùå Î™á Í∞ú Ï≤≠ÌÅ¨ Ï°∞Ìöå
        chunks_cursor = db.chunks.find(
            {"file_id": file_id}
        ).sort("sequence", 1).limit(chunk_count)
        
        chunks = await chunks_cursor.to_list(None)
        
        if not chunks:
            raise HTTPException(status_code=404, detail="ÌååÏùºÏùò Ï≤≠ÌÅ¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # Ï≤≠ÌÅ¨Îì§Ïùò ÌÖçÏä§Ìä∏Î•º Ìï©Ï≥êÏÑú ÎØ∏Î¶¨Î≥¥Í∏∞ ÏÉùÏÑ±
        preview_texts = [chunk["text"] for chunk in chunks]
        preview_text = "\n\n--- Îã§Ïùå ÏÑπÏÖò ---\n\n".join(preview_texts)
        
        # Ï†ÑÏ≤¥ Ï≤≠ÌÅ¨ Ïàò Ï°∞Ìöå
        total_chunks = await db.chunks.count_documents({"file_id": file_id})
        
        return {
            "file_id": file_id,
            "original_filename": document["original_filename"],
            "file_type": document["file_type"],
            "preview_text": preview_text,
            "preview_chunks": len(chunks),
            "total_chunks": total_chunks,
            "has_more": total_chunks > len(chunks),
            "preview_type": "chunks"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Ï≤≠ÌÅ¨ Í∏∞Î∞ò ÎØ∏Î¶¨Î≥¥Í∏∞ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 